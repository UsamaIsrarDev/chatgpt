{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fc0230",
   "metadata": {},
   "source": [
    "https://platform.openai.com/docs/guides/text-to-speech\n",
    "\n",
    "### Voice options\n",
    "\n",
    "Experiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) to find one that matches your desired tone and audience. The current voices are optimized for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f261f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m speech_file_path: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mspeech.mp3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m response = client.audio.speech.create(\n\u001b[32m      9\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mtts-1\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     voice=\u001b[33m'\u001b[39m\u001b[33malloy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mToday is a wonderful day to build something people love!\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\openai\\_client.py:130\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    128\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path: str = 'speech.mp3'\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model='tts-1',\n",
    "    voice='alloy',\n",
    "    input='Today is a wonderful day to build something people love!'\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e140bb6d",
   "metadata": {},
   "source": [
    "#### Supported languages\n",
    "\n",
    "The TTS model generally follows the Whisper model in terms of language support. Whisper supports the following languages and performs well despite the current voices being optimized for English:\n",
    "\n",
    "Afrikaans, Arabic, Armenian, Azerbaijani, Belarusian, Bosnian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kannada, Kazakh, Korean, Latvian, Lithuanian, Macedonian, Malay, Marathi, Maori, Nepali, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, Vietnamese, and Welsh.\n",
    "\n",
    "You can generate spoken audio in these languages by providing the input text in the language of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872868dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path = 'urdu.mp3'\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model='tts-1',\n",
    "    voice='alloy',\n",
    "    input='میرا نام محمد قاسم ہے'\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c41db",
   "metadata": {},
   "source": [
    "#### Create file 01_text-to-speech.py\n",
    "\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is a wonderful day to build something people love!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d044982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'g:\\\\git\\\\chatgpt\\\\10_multimodal_genai\\\\01_text-to-speech\\\\01_text-to-speech.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python 01_text-to-speech.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b57ed",
   "metadata": {},
   "source": [
    "### Streaming real time audio\n",
    "\n",
    "The Speech API provides support for real time audio streaming using chunk transfer encoding. This means that the audio is able to be played before the full file has been generated and made accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db922b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model='tts-1',\n",
    "    voice='alloy',\n",
    "    input='Hello World! This is a streaming test.'\n",
    ")\n",
    "\n",
    "response.stream_to_file('streaming.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff79bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
