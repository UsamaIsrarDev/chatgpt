{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1adda4",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Fine-tuning lets you get more out of the models available through the API by providing:\n",
    "\n",
    "Higher quality results than prompting Ability to train on more examples than can fit in a prompt Token savings due to shorter prompts Lower latency requests\n",
    "\n",
    "OpenAI's text generation models have been pre-trained on a vast amount of text. To use the models effectively, we include instructions and sometimes several examples in a prompt. Using demonstrations to show how to perform a task is often called \"few-shot learning.\"\n",
    "\n",
    "Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt. This saves costs and enables lower-latency requests.\n",
    "\n",
    "At a high level, fine-tuning involves the following steps:\n",
    "\n",
    "Prepare and upload training data Train a new fine-tuned model Evaluate results and go back to step 1 if needed Use your fine-tuned model\n",
    "\n",
    "### When to use fine-tuning\n",
    "\n",
    "Fine-tuning OpenAI text generation models can make them better for specific applications, but it requires a careful investment of time and effort. We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and function calling, with the key reasons being:\n",
    "\n",
    "* There are many tasks at which our models may not initially appear to perform well, but results can be improved with the right prompts - thus fine-tuning may not be necessary\n",
    "\n",
    "* Iterating over prompts and other tactics has a much faster feedback loop than iterating with fine-tuning, which requires creating datasets and running training jobs\n",
    "\n",
    "* In cases where fine-tuning is still necessary, initial prompt engineering work is not wasted - we typically see best results when using a good prompt in the fine-tuning data (or combining prompt chaining / tool use with fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5453bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isfile: path should be string, bytes, os.PathLike or integer, not function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m _: \u001b[38;5;28mbool\u001b[39m = \u001b[43mload_dotenv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_dotenv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:364\u001b[39m, in \u001b[36mload_dotenv\u001b[39m\u001b[34m(dotenv_path, stream, verbose, override, interpolate, encoding)\u001b[39m\n\u001b[32m    354\u001b[39m     dotenv_path = find_dotenv()\n\u001b[32m    356\u001b[39m dotenv = DotEnv(\n\u001b[32m    357\u001b[39m     dotenv_path=dotenv_path,\n\u001b[32m    358\u001b[39m     stream=stream,\n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m     encoding=encoding,\n\u001b[32m    363\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdotenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_as_environment_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:93\u001b[39m, in \u001b[36mDotEnv.set_as_environment_variables\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_as_environment_variables\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     90\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    Load the current dotenv as system environment variable.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dict().items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:76\u001b[39m, in \u001b[36mDotEnv.dict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m raw_values = \u001b[38;5;28mself\u001b[39m.parse()\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interpolate:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m._dict = OrderedDict(\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[43mresolve_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moverride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m._dict = OrderedDict(raw_values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:239\u001b[39m, in \u001b[36mresolve_variables\u001b[39m\u001b[34m(values, override)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve_variables\u001b[39m(\n\u001b[32m    234\u001b[39m     values: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]],\n\u001b[32m    235\u001b[39m     override: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    236\u001b[39m ) -> Mapping[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m    237\u001b[39m     new_values: Dict[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]] = {}\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:84\u001b[39m, in \u001b[36mDotEnv.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwith_warn_for_invalid_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Netever\\anaconda3\\envs\\python12\\Lib\\site-packages\\dotenv\\main.py:54\u001b[39m, in \u001b[36mDotEnv._get_stream\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[IO[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dotenv_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdotenv_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dotenv_path, encoding=\u001b[38;5;28mself\u001b[39m.encoding) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m stream\n",
      "\u001b[31mTypeError\u001b[39m: isfile: path should be string, bytes, os.PathLike or integer, not function"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_: bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client: OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d9578",
   "metadata": {},
   "source": [
    "### Preparing your dataset\n",
    "\n",
    "Once you have determined that fine-tuning is the right solution (i.e. you’ve optimized your prompt as far as it can take you and identified problems that the model still has), you’ll need to prepare data for training the model. You should create a diverse set of demonstration conversations that are similar to the conversations you will ask the model to respond to at inference time in production.\n",
    "\n",
    "#### Upload file for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f380685",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name:str = \"message.jsonl\"\n",
    "file_data = open(file_name, \"rb\")\n",
    "\n",
    "file = client.files.create(\n",
    "    file=file_data,\n",
    "    purpose=\"fine-tune\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e65d7",
   "metadata": {},
   "source": [
    "## Create a fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91209d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_model = client.fine_tuning.jobs.create(\n",
    "  training_file=file.id, \n",
    "  model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(fine_tune_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193faddf",
   "metadata": {},
   "source": [
    "It took more than an hour in my case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Retrieve the state of a fine-tune\n",
    "while True:\n",
    "    data = client.fine_tuning.jobs.retrieve(fine_tune_model.id)\n",
    "\n",
    "    if data.status == \"succeeded\":\n",
    "        break\n",
    "    elif data.status == \"failed\":\n",
    "        print(\"Fine-tune failed\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Fine-tune still running\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "\n",
    "def chat_completion(prompt : str )-> str:\n",
    " response : ChatCompletion = client.chat.completions.create(\n",
    "        model= \"ft:gpt-3.5-turbo-0613:personal::8TEbB8a7\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    " return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a19394",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Who wrote 'Romeo and Juliet'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"who was John F. Kennedy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162fb984",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"are you a fine-tuned model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"what is your purpose?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa133702",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Who was Robert Kennedy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22895a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Who killed Robert Kennedy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Who is Donald Trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Write some sentences as Donald Trump.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Write some sentences as Joe Biden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dde708",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion(\"Write some sentences as Imran Khan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884bda08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
